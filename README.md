# Tomoro-FinQA-Agent

## ğŸ§  Overview

**Tomoro-FinQA-Agent** is a Financial Question Answering (QA) agent that can:

- Interpret financial documents, including text and tables.
- Perform mathematical reasoning using a built-in calculator tool.
- Manage multi-turn conversations intelligently.
- Evaluate performance on datasets inspired by FinQA research.

The backend is powered by **FastAPI**, with supporting **Python scripts** for evaluation and dataset generation.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ .env.example
â”œâ”€â”€ .github
â”‚   â””â”€â”€ workflows
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent_components
â”‚   â”‚   â”œâ”€â”€ function_caller.py
â”‚   â”‚   â””â”€â”€ prompt_manager.py
â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ v1
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ routers
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â””â”€â”€ qa.py
â”‚   â”‚       â””â”€â”€ schemas
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â”œâ”€â”€ document_schemas.py
â”‚   â”‚           â””â”€â”€ qa_schemas.py
â”‚   â”œâ”€â”€ clients
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embedding_client.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â””â”€â”€ vector_db_client.py
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ domain_models.py
â”‚   â”œâ”€â”€ rag_pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ embedder.py
â”‚   â”‚   â”œâ”€â”€ indexer.py
â”‚   â”‚   â”œâ”€â”€ parser.py
â”‚   â”‚   â”œâ”€â”€ reranker.py
â”‚   â”‚   â””â”€â”€ retriever.py
â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ qa_service.py
â”‚   â”‚   â””â”€â”€ retrieval_service.py
â”‚   â””â”€â”€ tools
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ calculation_tool.py
â”‚       â”œâ”€â”€ knowledge_base_tool.py
â”‚       â”œâ”€â”€ query_financial_knowledge_base_tool.py
â”‚       â””â”€â”€ table_query_tool.py
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw_documents
â”‚   â”‚   â””â”€â”€ train.json
â”‚   â”œâ”€â”€ train.json
â”‚   â””â”€â”€ vector_store
â”‚       â”œâ”€â”€ faiss_index.idx
â”‚       â””â”€â”€ faiss_metadata.json
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ evaluation
â”‚   â”œâ”€â”€ compare_answers_testing.py
â”‚   â”œâ”€â”€ datasets
â”‚   â”‚   â”œâ”€â”€ prep
â”‚   â”‚   â”‚   â””â”€â”€ qa_eval_dataset.json
â”‚   â”‚   â””â”€â”€ qa_eval_dataset.json
â”‚   â”œâ”€â”€ results
â”‚   â”‚   â”œâ”€â”€ multi_turn_evaluation_summary_first_30.json
â”‚   â”‚   â”œâ”€â”€ single_shot_evaluation_summary_first_10.json
â”‚   â”‚   â”œâ”€â”€ single_shot_evaluation_summary_first_100_62.json
â”‚   â”‚   â”œâ”€â”€ single_shot_evaluation_summary_first_100_faulty_decimal.json
â”‚   â”‚   â””â”€â”€ single_shot_evaluation_summary_first_5.json
â”‚   â””â”€â”€ scripts
â”‚       â”œâ”€â”€ run_evaluation_single_shot.py
â”‚       â”œâ”€â”€ run_evaluation_single_shot_cost_latency.py
â”‚       â””â”€â”€ run_evaluation_turns.py
â”œâ”€â”€ prompts
â”‚   â”œâ”€â”€ financial_assistant_system_prompt.md
â”‚   â”œâ”€â”€ financial_assistant_system_prompt_v1.md
â”‚   â””â”€â”€ financial_assistant_system_prompt_v2.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements-dev.txt
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build_vector_store.py
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ prepare_evaluation_data.py
â”‚   â”œâ”€â”€ run_evaluations.py
â”‚   â””â”€â”€ test_parser.py
â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ e2e
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_build_vector_store.py
â”‚   â”‚   â””â”€â”€ test_full_qa_flow.py
â”‚   â”œâ”€â”€ integration
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_embedding_client.py
â”‚   â”‚   â”œâ”€â”€ test_example_integration.py
â”‚   â”‚   â””â”€â”€ test_llm_client.py
â”‚   â””â”€â”€ unit
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ agent_components
â”‚       â”‚   â”œâ”€â”€ test_function_caller.py
â”‚       â”‚   â””â”€â”€ test_prompt_manager.py
â”‚       â”œâ”€â”€ api
â”‚       â”‚   â””â”€â”€ test_qa_api.py
â”‚       â”œâ”€â”€ clients
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ test_llm_client.py
â”‚       â”œâ”€â”€ evaluation_pipeline
â”‚       â”‚   â””â”€â”€ evaluation_extraction.py
â”‚       â”œâ”€â”€ rag_pipeline
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ test_chunker.py
â”‚       â”‚   â”œâ”€â”€ test_indexer.py
â”‚       â”‚   â””â”€â”€ test_parser.py
â”‚       â”œâ”€â”€ services
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ test_qa_service.py
â”‚       â”‚   â””â”€â”€ test_retrieval_service.py
â”‚       â”œâ”€â”€ test_dataset_presence.py
â”‚       â”œâ”€â”€ test_example_unit.py
â”‚       â””â”€â”€ tools
â”‚           â”œâ”€â”€ test_calculation_tool.py
â”‚           â””â”€â”€ test_knowledge_base_tool.py
â””â”€â”€ uv.lock
```

---

## âš™ï¸ Setup & Installation

1. **Clone the Repository**

```bash
git clone https://github.com/AnalystTom/Tomoro-FinQA-Agent.git
cd Tomoro-FinQA-Agent
```

2. **Install `uv` (Fast Python Installer)**

Via pip:

```bash
pip install uv
```

Or via script:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

3. **Create and Activate Virtual Environment**

```bash
uv venv .venv
source .venv/bin/activate      # On Linux/macOS
# .venv\Scripts\activate       # On Windows
```

4. **Install Dependencies**

```bash
uv pip install -r requirements.txt
```

5. **Set Environment Variables**

```bash
cp .env.example .env
```

Then edit `.env` and set your OpenAI API key:

```env
OPENAI_API_KEY=your_api_key
```

---

## ğŸš€ Running the Application

Start the FastAPI server:

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

API Endpoint:

```
POST /api/v1/qa/process-query
```

---

## ğŸ“Š Evaluation Scripts

### ğŸŸ¢ Single-Shot Evaluation

```bash
python evaluation/scripts/run_evaluation_single_shot.py -n <num_items>
```

### ğŸ” Multi-Turn Evaluation

```bash
python evaluation/scripts/run_evaluation_turns.py -n <num_items>
```

---

## ğŸ§ª Running Tests

Run all unit tests with:

```bash
pytest
```

---

## ğŸ” Key Modules & Responsibilities

- **`app/main.py`, `api/`** â€“ FastAPI server setup and QA endpoints.
- **`qa_service.py`** â€“ Orchestrates dialogue, reasoning, and LLM tool usage.
- **`prompt_manager.py`** â€“ Loads system prompt and builds message history.
- **`function_caller.py`** â€“ Registers and executes tools (e.g., calculator).
- **`llm_client.py`** â€“ Manages async OpenAI API calls and function calling.
- **`parser.py`** â€“ Parses table data and converts to Markdown (legacy RAG).
- **`evaluation/scripts/`** â€“ Contains tools for dataset generation and evaluation workflows.

---

## âš ï¸ Known Issues

- **Missing Tool**: `query_table_by_cell_coordinates` is not implemented yet, affecting retrieval accuracy.
- **Evaluation Accuracy**: Heuristic for program output comparison needs improvements for structural robustness.
- **Answer Comparison**: Logic requires tuning for numeric and percentage-based formats.
